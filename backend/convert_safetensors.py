import os
import torch
from transformers import AutoModelForCausalLM
from safetensors.torch import load_file

# Define paths
MODEL_PATH = "backend/fine_tuned_model"
SAFE_FILE = os.path.join(MODEL_PATH, "model.safetensors")
BIN_FILE = os.path.join(MODEL_PATH, "pytorch_model.bin")

# Check if safetensors file exists
if not os.path.exists(SAFE_FILE):
    raise FileNotFoundError(f"âŒ ERROR: {SAFE_FILE} not found. Ensure model training completed successfully.")

# ğŸ”„ Step 1: Load pre-trained GPT-2 model
print("ğŸ”„ Initializing GPT-2 model...")
model = AutoModelForCausalLM.from_pretrained("gpt2")  # Ensure architecture is correct

# ğŸ”„ Step 2: Load safetensors
print(f"ğŸ”„ Loading weights from `{SAFE_FILE}`...")
state_dict = load_file(SAFE_FILE)

# ğŸ”„ Step 3: Apply weights to the model
missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)

# ğŸš¨ Check if keys are missing
if missing_keys:
    print(f"âš ï¸ Warning: Missing keys {missing_keys}. Model might not work correctly!")
if unexpected_keys:
    print(f"âš ï¸ Warning: Unexpected keys {unexpected_keys} found in state_dict!")

# ğŸ”„ Step 4: Save as PyTorch `.bin`
print("ğŸ’¾ Converting to `pytorch_model.bin`...")
torch.save(model.state_dict(), BIN_FILE)

print(f"âœ… Conversion complete! Model saved at `{BIN_FILE}`")
